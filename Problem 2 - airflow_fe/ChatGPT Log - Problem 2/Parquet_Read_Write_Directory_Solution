I have included this statement into my DAG script: def read_parquet(**kwargs):
    df = pd.read_parquet('C:\\Users\\knet9\\airflow_fe\\airflow-docker\\dags\\processed_data_fe.parquet')
    print(df)

which is contained within:
# -*- coding: utf-8 -*-
"""
Created on Wed May  3 17:50:10 2023

@author: knet9
"""
from kaggle.api.kaggle_api_extended import KaggleApi
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import os
import pandas as pd 
import pyarrow as pa
from pyarrow import parquet as pq
import glob



os.environ['KAGGLE_USERNAME'] = 'ampersandai' # kaggle_username = os.environ.get('KAGGLE_USERNAME')
os.environ['KAGGLE_KEY'] = 'f96702da2f2dff980e0d8ffa783e341f' # kaggle_key = os.environ.get('KAGGLE_KEY')

def download_dataset(**kwargs):
    api = KaggleApi()
    api.authenticate()
    api.dataset_download_files('jacksoncrow/stock-market-dataset', path='/tmp', unzip=True)

    # Get the list of all files in the /tmp directory
    all_files = glob.glob('/tmp/*.csv')

    # Push the list of files to be accessed by the next task
    kwargs['ti'].xcom_push(key='raw_data_files', value=all_files)

def process_dataset(**kwargs):
    all_files = kwargs['ti'].xcom_pull(key='raw_data_files')
    data = pd.DataFrame(columns=['Symbol', 'Security Name', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])
    
    for file in all_files:
        symbol = os.path.splitext(os.path.basename(file))[0]
        try:
            df = pd.read_csv(file)
            if 'Date' in df.columns:
                df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')  # <-- This line here
            else:
                print(f"File {file} does not contain a 'Date' column. Skipping...")
                continue
        except Exception as e:
            print(f"Error reading file {file}: {e}")
            continue
           
        
        df['Symbol'] = symbol
        df['Security Name'] = ''  # Add the security name here if available
        df = df[['Symbol', 'Security Name', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]
        
        data = data.append(df, ignore_index=True)
        
        # Enforce data types
    data = data.astype({
        'Symbol': 'str',
        'Security Name': 'str',
        'Date': 'str',
        'Open': 'float',
        'High': 'float',
        'Low': 'float',
        'Close': 'float',
        'Adj Close': 'float',
        'Volume': 'float'  # Change to 'int' if your volume data are always integers
    })

        
    data.to_csv('/tmp/processed_data.csv', index=False)
    kwargs['ti'].xcom_push(key='processed_data_path', value='/tmp/processed_data.csv')
                   
def FE(**kwargs):
    processed_data_path = kwargs['ti'].xcom_pull(key='processed_data_path')
    processed_data = pd.read_csv(processed_data_path, parse_dates=['Date'])
    if processed_data['Date'].dtypes != 'datetime64[ns]':
        processed_data['Date'] = pd.to_datetime(processed_data['Date']) 
    
    processed_data.sort_values(['Symbol', 'Date'], inplace = True)
    processed_data['vol_moving_avg'] = processed_data.groupby('Symbol')['Volume'].transform(lambda x: x.rolling(window=30).mean())
    processed_data['adj_close_rolling_med'] = processed_data.groupby('Symbol')['Adj Close'].transform(lambda x: x.rolling(window=30).median())

    processed_data.to_csv('/tmp/processed_data_fe.csv', index = False)
    kwargs['ti'].xcom_push(key='processed_data_fe_path', value = '/tmp/processed_data_fe.csv')


def convert_to_parquet(**kwargs):
    # Get the task instance
    ti = kwargs['ti']

    # Pull the input file path from XCom
    processed_data_fe_path = ti.xcom_pull(key='processed_data_fe_path')

    # Read the input file
    processed_data_fe = pd.read_csv(processed_data_fe_path, parse_dates=['Date'])

    # Convert the DataFrame to a PyArrow Table
    table = pa.Table.from_pandas(processed_data_fe)

    # Write the table to a Parquet file in the Linux subsystem directory
    pq.write_table(table, '/home/airflow/processed_data_fe.parquet')


def read_parquet(**kwargs):
    df = pd.read_parquet('C:\\Users\\knet9\\airflow_fe\\airflow-docker\\dags\\processed_data_fe.parquet')
    print(df)

            
default_args = {
    'owner':'airflow_fe',
    'depends_on_past': False, 
    'email_on_failure': False,
    'email_on_retry': False, 
    'retries': 2, 
    'retry_delay':timedelta(minutes=2),
    'start_date': datetime(2023, 4, 26)
}

dag = DAG(
    dag_id='stock_market_pipeline_fe',
    default_args=default_args, 
    description= 'Stock Market Pipeline FE',
    schedule_interval=timedelta(days=1),
    catchup=False,
)

download_task = PythonOperator(
   task_id='download_dataset',
   python_callable=download_dataset,
   provide_context=True,
   dag=dag,
)

process_task = PythonOperator(
    task_id='process_dataset',
    python_callable=process_dataset,
    provide_context=True,
    dag=dag,
)

fe_task = PythonOperator(
    task_id='feature_engineering',
    python_callable=FE,
    provide_context=True,
    dag=dag,
)

convert_task = PythonOperator(
    task_id='convert_to_parquet',
    python_callable=convert_to_parquet,
    provide_context=True,
    dag=dag,
)

read_task = PythonOperator(
    task_id='read_parquet',
    python_callable=convert_to_parquet,
    provide_context=True,
    dag=dag,
)

download_task >> process_task >> fe_task >> convert_task >> read_task

Does the file directory of where the DAG is being read make a significant different to the operations success?

ChatGPT
Yes, the file directory of where the DAG is being read can make a significant difference to the operation's success. In your DAG script, the read_parquet function reads a Parquet file using the file path 'C:\\Users\\knet9\\airflow_fe\\airflow-docker\\dags\\processed_data_fe.parquet'.

If the file directory specified in the file path is not accessible or the file does not exist in that directory, the pd.read_parquet operation will fail, and an error will be raised. The success of the operation depends on the availability and correctness of the file path.

Therefore, it is essential to ensure that the file directory specified in the file path is accurate and the Parquet file exists in that location for the read_parquet function to read the file successfully.





User
Since this is the directory where the table is being written:     pq.write_table(table, '/home/airflow/processed_data_fe.parquet')

the file would not be successfully printed within the python script correct?
ChatGPT
That's correct. In your DAG script, the Parquet file is being written to the directory '/home/airflow/processed_data_fe.parquet' using the line pq.write_table(table, '/home/airflow/processed_data_fe.parquet').

However, in the read_parquet function, you are attempting to read the Parquet file from a different directory using the line df = pd.read_parquet('C:\\Users\\knet9\\airflow_fe\\airflow-docker\\dags\\processed_data_fe.parquet').

If the Parquet file is written to '/home/airflow/processed_data_fe.parquet', but you are trying to read it from a different directory, namely 'C:\\Users\\knet9\\airflow_fe\\airflow-docker\\dags\\processed_data_fe.parquet', the file will not be found, and the pd.read_parquet operation will fail.

To successfully read the Parquet file, you should update the file path in the read_parquet function to '/home/airflow/processed_data_fe.parquet' to match the directory where the file is being written.
