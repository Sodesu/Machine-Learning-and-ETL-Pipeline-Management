# -*- coding: utf-8 -*-
"""
Created on Wed May  3 17:50:10 2023

@author: knet9
"""
from kaggle.api.kaggle_api_extended import KaggleApi
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import os
import pandas as pd 
import pyarrow as pa
from pyarrow import parquet as pq
import glob

os.environ['KAGGLE_USERNAME'] = 'ampersandai' # kaggle_username = os.environ.get('KAGGLE_USERNAME')
os.environ['KAGGLE_KEY'] = 'f96702da2f2dff980e0d8ffa783e341f' # kaggle_key = os.environ.get('KAGGLE_KEY')

def download_dataset(**kwargs):
    api = KaggleApi()
    api.authenticate()
    api.dataset_download_files('jacksoncrow/stock-market-dataset', path='/tmp', unzip=True)

    # Get the list of all files in the /tmp directory
    all_files = glob.glob('/tmp/*.csv')

    # Push the list of files to be accessed by the next task
    kwargs['ti'].xcom_push(key='raw_data_files', value=all_files)

def process_dataset(**kwargs):
    all_files = kwargs['ti'].xcom_pull(key='raw_data_files')
    data = pd.DataFrame(columns=['Symbol', 'Security Name', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'])
    
    for file in all_files:
        symbol = os.path.splitext(os.path.basename(file))[0]
        try:
            df = pd.read_csv(file)
            if 'Date' in df.columns:
                df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')  # <-- This line here
            else:
                print(f"File {file} does not contain a 'Date' column. Skipping...")
                continue
        except Exception as e:
            print(f"Error reading file {file}: {e}")
            continue
           
        
        df['Symbol'] = symbol
        df['Security Name'] = ''  # Add the security name here if available
        df = df[['Symbol', 'Security Name', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]
        
        data = data.append(df, ignore_index=True)
        
        # Enforce data types
    data = data.astype({
        'Symbol': 'str',
        'Security Name': 'str',
        'Date': 'str',
        'Open': 'float',
        'High': 'float',
        'Low': 'float',
        'Close': 'float',
        'Adj Close': 'float',
        'Volume': 'float'  # Change to 'int' if your volume data are always integers
    })

        
    data.to_csv('/tmp/processed_data.csv', index=False)
    kwargs['ti'].xcom_push(key='processed_data_path', value='/tmp/processed_data.csv')
                   
def FE(**kwargs):
    processed_data_path = kwargs['ti'].xcom_pull(key='processed_data_path')
    processed_data = pd.read_csv(processed_data_path, parse_dates=['Date'])
    if processed_data['Date'].dtypes != 'datetime64[ns]':
        processed_data['Date'] = pd.to_datetime(processed_data['Date']) 
    
    processed_data.sort_values(['Symbol', 'Date'], inplace = True)
    processed_data['vol_moving_avg'] = processed_data.groupby('Symbol')['Volume'].transform(lambda x: x.rolling(window=30).mean())
    processed_data['adj_close_rolling_med'] = processed_data.groupby('Symbol')['Adj Close'].transform(lambda x: x.rolling(window=30).median())

    processed_data.to_csv('/tmp/processed_data_fe.csv', index = False)
    kwargs['ti'].xcom_push(key='processed_data_fe_path', value = '/tmp/processed_data_fe.csv')


def convert_to_parquet(**kwargs):
    # Get the task instance
    ti = kwargs['ti']

    # Pull the input file path from XCom
    processed_data_fe_path = ti.xcom_pull(key='processed_data_fe_path')

    # Read the input file
    processed_data_fe = pd.read_csv(processed_data_fe_path, parse_dates=['Date'])

    # Convert the DataFrame to a PyArrow Table
    table = pa.Table.from_pandas(processed_data_fe)

    # Write the table to a Parquet file in the Linux subsystem directory
    pq.write_table(table, '/home/airflow/processed_data_fe.parquet')


def read_parquet(**kwargs):
    df = pd.read_parquet('/home/airflow/processed_data_fe.parquet')
    print(df)


            
default_args = {
    'owner':'airflow_fe',
    'depends_on_past': False, 
    'email_on_failure': False,
    'email_on_retry': False, 
    'retries': 3, 
    'retry_delay':timedelta(minutes=2),
    'start_date': datetime(2023, 4, 26)
}

dag = DAG(
    dag_id='stock_market_pipeline_fe',
    default_args=default_args, 
    description= 'Stock Market Pipeline FE',
    schedule_interval=timedelta(days=1),
    catchup=False,
)

download_task = PythonOperator(
   task_id='download_dataset',
   python_callable=download_dataset,
   provide_context=True,
   dag=dag,
)

process_task = PythonOperator(
    task_id='process_dataset',
    python_callable=process_dataset,
    provide_context=True,
    dag=dag,
)

fe_task = PythonOperator(
    task_id='feature_engineering',
    python_callable=FE,
    provide_context=True,
    dag=dag,
)

convert_task = PythonOperator(
    task_id='convert_to_parquet',
    python_callable=convert_to_parquet,
    provide_context=True,
    dag=dag,
)

read_task = PythonOperator(
    task_id='read_parquet',
    python_callable=read_parquet,
    provide_context=True,
    dag=dag,
)


download_task >> process_task >> fe_task >> convert_task >> read_task 
